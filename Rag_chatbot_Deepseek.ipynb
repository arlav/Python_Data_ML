{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPwj2QUXqFXT"
      },
      "source": [
        "## ChatBot code, a simple UI and a logging file for each dialogue session. It also includes a basic Retrieval Augmented Generation part that uses a file you will upload. \n",
        "### Using DeepSeek with Ollama locally instead of OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NgdtkqqMXnbV"
      },
      "outputs": [],
      "source": [
        "# adding the required libraries\n",
        "%%capture\n",
        "!pip install langchain-community tiktoken pdfplumber ipywidgets faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gxt2lYqC0StB"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pdfplumber #for data extraction from the PDf\n",
        "import tiktoken #for tokenising text\n",
        "import time\n",
        "import textwrap #for interface\n",
        "import ipywidgets as widgets #for interface\n",
        "import IPython                                                          #Interactive Python Shell\n",
        "from IPython.display import display, Markdown\n",
        "import requests #for API calls to Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xED8e6XA0uHt"
      },
      "outputs": [],
      "source": [
        "# LangChain imports\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings         #text to numerical vectors - embeddings\n",
        "from langchain.vectorstores import FAISS                                 #similarity search for vectors\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter       #splits large text chunks into smaller\n",
        "from langchain_community.llms import Ollama                              #imports Ollama integration\n",
        "from langchain.chains import RetrievalQA                                 #pre-built chain for document retrieval and question answering\n",
        "from langchain.prompts import PromptTemplate                             #PromptTemplates for LangChain - Persona, Task, Communication\n",
        "from langchain.memory import ConversationBufferMemory                    #Memory Handling for LangChain\n",
        "from langchain.schema.runnable import RunnableMap, RunnableSequence      #Schema mapping and sequence for LangChain\n",
        "from langchain.schema.output_parser import StrOutputParser               #Output parser for LangChain\n",
        "from langchain_community.chat_models import ChatOllama                   #Chat model for Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9OmZyGt0JaE6"
      },
      "outputs": [],
      "source": [
        "# Ollama API setup\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434\" # Default Ollama URL\n",
        "# If using a remote Ollama server, change the URL above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9AuWTy9x1SIX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DeepSeek model(s) available: ['deepseek-r1:latest', 'deepseek-coder-v2:latest', 'deepseek-coder:latest']\n"
          ]
        }
      ],
      "source": [
        "# Verify Ollama is running and has DeepSeek model pulled\n",
        "def check_ollama():\n",
        "    try:\n",
        "        # Check if Ollama is running\n",
        "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
        "        if response.status_code != 200:\n",
        "            print(\"Error: Ollama server is not running. Please start Ollama.\")\n",
        "            return False\n",
        "            \n",
        "        # Check if DeepSeek model is available\n",
        "        models = response.json().get(\"models\", [])\n",
        "        deepseek_models = [model for model in models if \"deepseek\" in model[\"name\"].lower()]\n",
        "        \n",
        "        if not deepseek_models:\n",
        "            print(\"DeepSeek model not found. Pulling the model (this may take some time)...\")\n",
        "            # Pull the model\n",
        "            pull_response = requests.post(\n",
        "                f\"{OLLAMA_BASE_URL}/api/pull\",\n",
        "                json={\"name\": \"deepseek-coder:6.7b\"}\n",
        "            )\n",
        "            if pull_response.status_code != 200:\n",
        "                print(f\"Error pulling model: {pull_response.text}\")\n",
        "                return False\n",
        "            print(\"DeepSeek model pulled successfully.\")\n",
        "        else:\n",
        "            print(f\"DeepSeek model(s) available: {[model['name'] for model in deepseek_models]}\")\n",
        "        return True\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"Error: Cannot connect to Ollama server. Please make sure Ollama is running.\")\n",
        "        return False\n",
        "\n",
        "# Run the check\n",
        "ollama_ready = check_ollama()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y2f6paXo-w_z"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1C8tI7481_tY"
      },
      "outputs": [],
      "source": [
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text content from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            extracted = page.extract_text()\n",
        "            if extracted:  # Avoid NoneType errors\n",
        "                text += extracted + \"\\n\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zjUfHdi6w78E"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named '_tkinter'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Try Google Colab approach first\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease upload your PDF document:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# If not in Colab, provide a local file path option\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtk\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filedialog\n\u001b[1;32m     14\u001b[0m     root \u001b[38;5;241m=\u001b[39m tk\u001b[38;5;241m.\u001b[39mTk()\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/tkinter/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_tkinter\u001b[39;00m \u001b[38;5;66;03m# If this fails your Python may not be configured for Tk\u001b[39;00m\n\u001b[1;32m     39\u001b[0m TclError \u001b[38;5;241m=\u001b[39m _tkinter\u001b[38;5;241m.\u001b[39mTclError\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_tkinter'"
          ]
        }
      ],
      "source": [
        "# Upload PDF file\n",
        "try:\n",
        "    # Try Google Colab approach first\n",
        "    from google.colab import files\n",
        "    print(\"Please upload your PDF document:\")\n",
        "    uploaded = files.upload()\n",
        "    pdf_filename = list(uploaded.keys())[0]\n",
        "    pdf_path = f\"/content/{pdf_filename}\"\n",
        "except ImportError:\n",
        "    # If not in Colab, provide a local file path option\n",
        "    import tkinter as tk\n",
        "    from tkinter import filedialog\n",
        "    \n",
        "    root = tk.Tk()\n",
        "    root.withdraw()\n",
        "    \n",
        "    print(\"Please select your PDF document:\")\n",
        "    pdf_path = filedialog.askopenfilename(filetypes=[(\"PDF Files\", \"*.pdf\")])\n",
        "    if pdf_path:\n",
        "        pdf_filename = os.path.basename(pdf_path)\n",
        "    else:\n",
        "        print(\"No file selected. Please run this cell again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVp4tX642W0P"
      },
      "outputs": [],
      "source": [
        "# Extract text from the uploaded PDF\n",
        "try:\n",
        "    pdf_text = extract_text_from_pdf(pdf_path)\n",
        "    print(f\"Successfully extracted text from {pdf_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error extracting text from PDF: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8Vk4-ewt2bZY"
      },
      "outputs": [],
      "source": [
        "# Split the document into chunks for embedding\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "documents = text_splitter.create_documents([pdf_text])\n",
        "\n",
        "# Create vector embeddings and store in FAISS\n",
        "# Using Hugging Face embeddings instead of OpenAI embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Initialize the chat model using Ollama with DeepSeek\n",
        "llm = ChatOllama(\n",
        "    model=\"deepseek-coder:6.7b\",  # DeepSeek coder model\n",
        "    temperature=0.7,\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    streaming=False  # Set to True if you want streaming responses\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4O9U2V4V2eiY"
      },
      "outputs": [],
      "source": [
        "# Create the RAG (Retrieval Augmented Generation) chain\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "H78KcZyw2fFp"
      },
      "outputs": [],
      "source": [
        "# Define the prompt template for our Topologic ChatBot\n",
        "#customise the Persona, Task, Communication, Context as needed. test whether deep/specific details work better.                                              \n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "<Persona>\n",
        "You are a very technical python coder with expertise in geometry and topology, and in particular topologicPy and Industrial Foundation Classes strategies.\n",
        "</Persona>\n",
        "\n",
        "<Task>\n",
        "The conversation is about helping junior python coders to develop code using Topologicpy API\n",
        "Please use few-shot strategy to benchmark your responses when the questions are difficult.\n",
        "Use the retrieved context when it's relevant to answer the user's question.\n",
        "Communicate sources for your answers when needed.\n",
        "</Task>\n",
        "\n",
        "<Communication>\n",
        "Respond in detailed python code and with deatialed comments and explanations.\n",
        "Keep the dialogue on track.\n",
        "Never reveal you are an AI or LLM.\n",
        "If questioned further provide an explanation of at least one paragraph with ten sentences as an explanation of your thinking.\n",
        "Ensure your answers are data-driven when possible, drawing from the context provided.\n",
        "</Communication>\n",
        "\n",
        "<Context>\n",
        "{context}\n",
        "</Context>\n",
        "\n",
        "Conversation history:\n",
        "{history}\n",
        "\n",
        "User: {user_input}\n",
        "ChatBot:\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next part develops the interface. The chat returns input after hitting return on the keyboard. Be patient with it as it might take time since we're running DeepSeek locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxe3bzqM2iY0"
      },
      "outputs": [],
      "source": [
        "# Initialize memory for conversation history\n",
        "memory = ConversationBufferMemory(return_messages=True, max_token_limit=500)\n",
        "\n",
        "# Create a function to process user input using both RAG and the conversational prompt\n",
        "def process_user_input(user_input, history):\n",
        "    # First, use RAG to retrieve relevant context\n",
        "    rag_response = rag_chain({\"query\": user_input})\n",
        "    relevant_context = rag_response.get(\"result\", \"\")\n",
        "\n",
        "    # Format the conversation history\n",
        "    formatted_history = \"\\n\".join([f\"User: {h['user_input']}\\nChatBot: {h['assistant']}\" for h in history])\n",
        "\n",
        "    # Use the prompt template with the retrieved context\n",
        "    response = llm.invoke(prompt_template.format(\n",
        "        context=relevant_context,\n",
        "        history=formatted_history,\n",
        "        user_input=user_input\n",
        "    ))\n",
        "\n",
        "    return response.content\n",
        "\n",
        "# UI Elements\n",
        "chat_output = widgets.Output()\n",
        "user_input_box = widgets.Textarea(\n",
        "    placeholder=\"Enter your message here...\",\n",
        "    description=\"User:\",\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width=\"80%\", height=\"50px\")\n",
        ")\n",
        "end_chat_button = widgets.Button(description=\"End Chat Session\", button_style=\"danger\")\n",
        "\n",
        "# Display UI Elements\n",
        "display(chat_output, user_input_box, end_chat_button)\n",
        "\n",
        "# Initialize conversation history\n",
        "history = []\n",
        "\n",
        "# Open log file in append mode\n",
        "log_filename = \"rag_chat_history.txt\"\n",
        "log_file = open(log_filename, \"a\", encoding=\"utf-8\")\n",
        "\n",
        "# Show initial message\n",
        "with chat_output:\n",
        "    print(\"Welcome! I'm your TopologicPy assistant. How can I help you today?\")\n",
        "\n",
        "def handle_input():\n",
        "    \"\"\"Handles user input when Enter is pressed.\"\"\"\n",
        "    user_input = user_input_box.value.strip()\n",
        "\n",
        "    if not user_input:\n",
        "        return  # Ignore empty input\n",
        "\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        stop_chat()\n",
        "        return\n",
        "\n",
        "    # Display user input\n",
        "    with chat_output:\n",
        "        print(f\"User: {user_input}\")\n",
        "\n",
        "    # Process response using combined approach\n",
        "    try:\n",
        "        response_text = process_user_input(user_input, history)\n",
        "        wrapped_response = textwrap.fill(response_text, width=120)\n",
        "\n",
        "        with chat_output:\n",
        "            print(f\"ChatBot: {wrapped_response}\")\n",
        "\n",
        "        # Update conversation history\n",
        "        history.append({\"user_input\": user_input, \"assistant\": response_text})\n",
        "\n",
        "        # Log conversation to file\n",
        "        log_file.write(f\"User: {user_input}\\n\")\n",
        "        log_file.write(f\"ChatBot: {response_text}\\n\\n\")\n",
        "        log_file.flush()  # Ensure data is written immediately\n",
        "\n",
        "    except Exception as e:\n",
        "        with chat_output:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    # Clear input box for next message\n",
        "    user_input_box.value = \"\"\n",
        "\n",
        "def handle_keypress(change):\n",
        "    \"\"\"Detect Enter and submit input.\"\"\"\n",
        "    if change[\"name\"] == \"value\" and change[\"new\"].endswith(\"\\n\"):  # Detect newlines\n",
        "        handle_input()\n",
        "\n",
        "def stop_chat(_=None):\n",
        "    \"\"\"Ends the chat, saves dialogue history and closes the log file\"\"\"\n",
        "    global log_file\n",
        "    log_file.close()  # Close file properly\n",
        "\n",
        "    with chat_output:\n",
        "        print(\"\\nGoodbye! Chat history saved to 'rag_chat_history.txt'.\")\n",
        "\n",
        "    disable_input()\n",
        "\n",
        "def disable_input():\n",
        "    \"\"\"Disables input box and chat button after chat ends.\"\"\"\n",
        "    user_input_box.close()\n",
        "    end_chat_button.disabled = True\n",
        "\n",
        "# Bind buttons and input events\n",
        "end_chat_button.on_click(stop_chat)\n",
        "\n",
        "# Attach event listener for Enter\n",
        "user_input_box.observe(handle_keypress, names=\"value\")\n",
        "\n",
        "# Function to analyze token usage (for debugging/optimization)\n",
        "def count_tokens(text, encoding_name=\"cl100k_base\"):\n",
        "    try:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        return len(encoding.encode(text))\n",
        "    except:\n",
        "        # Fallback method if tiktoken doesn't support the encoding\n",
        "        return len(text.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQCNuaeHgrMy"
      },
      "source": [
        "## Additional Information\n",
        "\n",
        "### Setting up Ollama\n",
        "\n",
        "Before running this notebook, you need to have Ollama installed and running on your system. Here's how to set it up:\n",
        "\n",
        "1. Install Ollama from [https://ollama.ai/](https://ollama.ai/)\n",
        "2. Start the Ollama service\n",
        "3. Pull the DeepSeek model using this command in your terminal:\n",
        "   ```\n",
        "   ollama pull deepseek-coder:6.7b\n",
        "   ```\n",
        "\n",
        "### Using Different DeepSeek Models\n",
        "\n",
        "Ollama supports various DeepSeek models. You can change the model by modifying the `model` parameter in the `ChatOllama` initialization. Some options include:\n",
        "\n",
        "- `deepseek-coder:6.7b` - A smaller coding-focused model\n",
        "- `deepseek-coder:33b` - A larger coding-focused model (requires more RAM)\n",
        "- `deepseek-llm:7b` - A general-purpose model\n",
        "\n",
        "Check Ollama's model library for the latest available models.\n",
        "\n",
        "### LangChain Documentation\n",
        "\n",
        "For more information on using LangChain with Ollama, see the documentation at:\n",
        "https://python.langchain.com/docs/integrations/llms/ollama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWTQ17QN5l2b"
      },
      "source": [
        "### Note: This RAG implementation uses:\n",
        "- DeepSeek model with Ollama for local inference\n",
        "- Hugging Face Sentence Transformers for embeddings\n",
        "- FAISS for vector similarity search\n",
        "- LangChain for RAG workflow management\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
